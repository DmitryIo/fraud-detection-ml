[2022-08-21 19:49:16,402] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: preprocessing_data.clean_data scheduled__2022-08-21T19:39:13.291295+00:00 [queued]>
[2022-08-21 19:49:16,406] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: preprocessing_data.clean_data scheduled__2022-08-21T19:39:13.291295+00:00 [queued]>
[2022-08-21 19:49:16,406] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2022-08-21 19:49:16,406] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2022-08-21 19:49:16,406] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2022-08-21 19:49:16,419] {taskinstance.py:1397} INFO - Executing <Task(SparkSubmitOperatorXCom): clean_data> on 2022-08-21 19:39:13.291295+00:00
[2022-08-21 19:49:16,422] {standard_task_runner.py:52} INFO - Started process 27175 to run task
[2022-08-21 19:49:16,426] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'preprocessing_data', 'clean_data', 'scheduled__2022-08-21T19:39:13.291295+00:00', '--job-id', '22', '--raw', '--subdir', 'DAGS_FOLDER/model_dag.py', '--cfg-path', '/tmp/tmpymd01rnq', '--error-file', '/tmp/tmpd4kjhldq']
[2022-08-21 19:49:16,426] {standard_task_runner.py:80} INFO - Job 22: Subtask clean_data
[2022-08-21 19:49:16,560] {task_command.py:371} INFO - Running <TaskInstance: preprocessing_data.clean_data scheduled__2022-08-21T19:39:13.291295+00:00 [running]> on host rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net
[2022-08-21 19:49:16,602] {taskinstance.py:1589} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@example.com
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=preprocessing_data
AIRFLOW_CTX_TASK_ID=clean_data
AIRFLOW_CTX_EXECUTION_DATE=2022-08-21T19:39:13.291295+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-08-21T19:39:13.291295+00:00
[2022-08-21 19:49:16,606] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2022-08-21 19:49:16,607] {spark_submit.py:334} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /home/ubuntu/fraud-detection-ml/utils/pyspark_cleaning.py
[2022-08-21 19:49:17,739] {spark_submit.py:485} INFO - SLF4J: Class path contains multiple SLF4J bindings.
[2022-08-21 19:49:17,739] {spark_submit.py:485} INFO - SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[2022-08-21 19:49:17,739] {spark_submit.py:485} INFO - SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[2022-08-21 19:49:17,739] {spark_submit.py:485} INFO - SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
[2022-08-21 19:49:17,739] {spark_submit.py:485} INFO - SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[2022-08-21 19:49:19,169] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,169 INFO spark.SparkContext: Running Spark version 3.0.3
[2022-08-21 19:49:19,208] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,207 INFO resource.ResourceUtils: ==============================================================
[2022-08-21 19:49:19,209] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,209 INFO resource.ResourceUtils: Resources for spark.driver:
[2022-08-21 19:49:19,209] {spark_submit.py:485} INFO - 
[2022-08-21 19:49:19,209] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,209 INFO resource.ResourceUtils: ==============================================================
[2022-08-21 19:49:19,209] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,209 INFO spark.SparkContext: Submitted application: arrow-spark
[2022-08-21 19:49:19,269] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,269 INFO spark.SecurityManager: Changing view acls to: ubuntu
[2022-08-21 19:49:19,269] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,269 INFO spark.SecurityManager: Changing modify acls to: ubuntu
[2022-08-21 19:49:19,270] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,269 INFO spark.SecurityManager: Changing view acls groups to:
[2022-08-21 19:49:19,270] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,270 INFO spark.SecurityManager: Changing modify acls groups to:
[2022-08-21 19:49:19,270] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,270 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
[2022-08-21 19:49:19,494] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,489 INFO util.Utils: Successfully started service 'sparkDriver' on port 33849.
[2022-08-21 19:49:19,524] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,523 INFO spark.SparkEnv: Registering MapOutputTracker
[2022-08-21 19:49:19,563] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,563 INFO spark.SparkEnv: Registering BlockManagerMaster
[2022-08-21 19:49:19,584] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,583 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-08-21 19:49:19,584] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,584 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-08-21 19:49:19,613] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,613 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-08-21 19:49:19,627] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,627 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-81e85245-e08f-426c-8008-410a54be67c4
[2022-08-21 19:49:19,665] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,661 INFO memory.MemoryStore: MemoryStore started with capacity 1643.3 MiB
[2022-08-21 19:49:19,700] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,699 INFO spark.SparkEnv: Registering OutputCommitCoordinator
[2022-08-21 19:49:19,854] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,854 INFO util.log: Logging initialized @3008ms to org.sparkproject.jetty.util.log.Slf4jLog
[2022-08-21 19:49:19,948] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,948 INFO server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07
[2022-08-21 19:49:19,982] {spark_submit.py:485} INFO - 2022-08-21 19:49:19,982 INFO server.Server: Started @3137ms
[2022-08-21 19:49:20,036] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,036 INFO server.AbstractConnector: Started ServerConnector@3a673000{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
[2022-08-21 19:49:20,036] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,036 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
[2022-08-21 19:49:20,062] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bef19e9{/jobs,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,065] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,065 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f1fc05c{/jobs/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,066] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,066 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@333c67af{/jobs/job,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,067] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,067 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c567462{/jobs/job/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,068] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,068 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49c0481{/stages,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,069] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,069 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33fba7d5{/stages/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,070] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,070 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@501d9950{/stages/stage,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,072] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,072 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a956636{/stages/stage/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,073] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,073 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aed57fe{/stages/pool,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,074] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@87048f3{/stages/pool/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,075] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,075 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@476f34ff{/storage,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,076] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d0e586d{/storage/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,076] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aa67ff5{/storage/rdd,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,077] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,077 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f50b037{/storage/rdd/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,078] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,078 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61bd0f35{/environment,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,079] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,078 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@190d3042{/environment/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,079] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,079 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@247922b1{/executors,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,080] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,080 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10cca46e{/executors/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,081] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,081 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72520f6f{/executors/threadDump,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,081] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,081 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f3b7716{/executors/threadDump/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,092] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,092 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c76049a{/static,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,093] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,093 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d8e963{/,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,094] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,094 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68d6ca95{/api,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,095] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,095 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f80efa{/jobs/job/kill,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,099] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,099 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2162dbc0{/stages/stage/kill,null,AVAILABLE,@Spark}
[2022-08-21 19:49:20,101] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,101 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:4040
[2022-08-21 19:49:20,294] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,294 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
[2022-08-21 19:49:20,296] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,296 INFO util.Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
[2022-08-21 19:49:20,368] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,368 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net/10.129.0.19:8032
[2022-08-21 19:49:20,524] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,524 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net/10.129.0.19:10200
[2022-08-21 19:49:20,606] {spark_submit.py:485} INFO - 2022-08-21 19:49:20,606 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers
[2022-08-21 19:49:21,120] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,120 INFO conf.Configuration: resource-types.xml not found
[2022-08-21 19:49:21,121] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,121 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
[2022-08-21 19:49:21,133] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,133 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
[2022-08-21 19:49:21,133] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,133 INFO yarn.Client: Will allocate AM container, with 3456 MB memory including 384 MB overhead
[2022-08-21 19:49:21,134] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,133 INFO yarn.Client: Setting up container launch context for our AM
[2022-08-21 19:49:21,135] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,135 INFO yarn.Client: Setting up the launch environment for our AM container
[2022-08-21 19:49:21,146] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,146 INFO yarn.Client: Preparing resources for our AM container
[2022-08-21 19:49:21,184] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,184 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net/user/ubuntu/.sparkStaging/application_1661098118612_0008/pyspark.zip
[2022-08-21 19:49:21,379] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,379 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net/user/ubuntu/.sparkStaging/application_1661098118612_0008/py4j-0.10.9-src.zip
[2022-08-21 19:49:21,569] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,569 INFO yarn.Client: Uploading resource file:/tmp/spark-d80d3c76-e708-4e9a-9618-ad111728fc5c/__spark_conf__8900563592481575994.zip -> hdfs://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net/user/ubuntu/.sparkStaging/application_1661098118612_0008/__spark_conf__.zip
[2022-08-21 19:49:21,636] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,636 INFO spark.SecurityManager: Changing view acls to: ubuntu
[2022-08-21 19:49:21,636] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,636 INFO spark.SecurityManager: Changing modify acls to: ubuntu
[2022-08-21 19:49:21,636] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,636 INFO spark.SecurityManager: Changing view acls groups to:
[2022-08-21 19:49:21,636] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,636 INFO spark.SecurityManager: Changing modify acls groups to:
[2022-08-21 19:49:21,636] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,636 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
[2022-08-21 19:49:21,663] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,661 INFO yarn.Client: Submitting application application_1661098118612_0008 to ResourceManager
[2022-08-21 19:49:21,702] {spark_submit.py:485} INFO - 2022-08-21 19:49:21,702 INFO impl.YarnClientImpl: Submitted application application_1661098118612_0008
[2022-08-21 19:49:22,706] {spark_submit.py:485} INFO - 2022-08-21 19:49:22,706 INFO yarn.Client: Application report for application_1661098118612_0008 (state: ACCEPTED)
[2022-08-21 19:49:22,708] {spark_submit.py:485} INFO - 2022-08-21 19:49:22,708 INFO yarn.Client:
[2022-08-21 19:49:22,708] {spark_submit.py:485} INFO - client token: N/A
[2022-08-21 19:49:22,708] {spark_submit.py:485} INFO - diagnostics: AM container is launched, waiting for AM container to Register with RM
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - ApplicationMaster host: N/A
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - ApplicationMaster RPC port: -1
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - queue: default
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - start time: 1661111361674
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - final status: UNDEFINED
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - tracking URL: http://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:8088/proxy/application_1661098118612_0008/
[2022-08-21 19:49:22,709] {spark_submit.py:485} INFO - user: ubuntu
[2022-08-21 19:49:23,711] {spark_submit.py:485} INFO - 2022-08-21 19:49:23,710 INFO yarn.Client: Application report for application_1661098118612_0008 (state: ACCEPTED)
[2022-08-21 19:49:24,216] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,216 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net, PROXY_URI_BASES -> http://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:8088/proxy/application_1661098118612_0008), /proxy/application_1661098118612_0008
[2022-08-21 19:49:24,713] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,713 INFO yarn.Client: Application report for application_1661098118612_0008 (state: RUNNING)
[2022-08-21 19:49:24,713] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,713 INFO yarn.Client:
[2022-08-21 19:49:24,713] {spark_submit.py:485} INFO - client token: N/A
[2022-08-21 19:49:24,713] {spark_submit.py:485} INFO - diagnostics: N/A
[2022-08-21 19:49:24,713] {spark_submit.py:485} INFO - ApplicationMaster host: 10.129.0.18
[2022-08-21 19:49:24,714] {spark_submit.py:485} INFO - ApplicationMaster RPC port: -1
[2022-08-21 19:49:24,714] {spark_submit.py:485} INFO - queue: default
[2022-08-21 19:49:24,714] {spark_submit.py:485} INFO - start time: 1661111361674
[2022-08-21 19:49:24,714] {spark_submit.py:485} INFO - final status: UNDEFINED
[2022-08-21 19:49:24,714] {spark_submit.py:485} INFO - tracking URL: http://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:8088/proxy/application_1661098118612_0008/
[2022-08-21 19:49:24,714] {spark_submit.py:485} INFO - user: ubuntu
[2022-08-21 19:49:24,715] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,715 INFO cluster.YarnClientSchedulerBackend: Application application_1661098118612_0008 has started running.
[2022-08-21 19:49:24,737] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,736 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33851.
[2022-08-21 19:49:24,738] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,737 INFO netty.NettyBlockTransferService: Server created on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851
[2022-08-21 19:49:24,740] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,739 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-08-21 19:49:24,760] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,760 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net, 33851, None)
[2022-08-21 19:49:24,764] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,764 INFO storage.BlockManagerMasterEndpoint: Registering block manager rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 with 1643.3 MiB RAM, BlockManagerId(driver, rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net, 33851, None)
[2022-08-21 19:49:24,768] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,767 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net, 33851, None)
[2022-08-21 19:49:24,768] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,768 INFO storage.BlockManager: external shuffle service port = 7337
[2022-08-21 19:49:24,770] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,770 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net, 33851, None)
[2022-08-21 19:49:24,811] {spark_submit.py:485} INFO - 2022-08-21 19:49:24,810 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
[2022-08-21 19:49:25,004] {spark_submit.py:485} INFO - 2022-08-21 19:49:25,004 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[2022-08-21 19:49:25,008] {spark_submit.py:485} INFO - 2022-08-21 19:49:25,008 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb88ac8{/metrics/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:25,046] {spark_submit.py:485} INFO - 2022-08-21 19:49:25,046 INFO history.SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1661098118612_0008.inprogress
[2022-08-21 19:49:25,267] {spark_submit.py:485} INFO - 2022-08-21 19:49:25,267 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
[2022-08-21 19:49:25,268] {spark_submit.py:485} INFO - 2022-08-21 19:49:25,267 INFO util.Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
[2022-08-21 19:49:26,967] {spark_submit.py:485} INFO - 2022-08-21 19:49:26,967 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 5530, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-08-21 19:49:27,610] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,610 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.129.0.18:41230) with ID 1
[2022-08-21 19:49:27,620] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,620 INFO dynalloc.ExecutorMonitor: New executor 1 has registered (new total is 1)
[2022-08-21 19:49:27,631] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,631 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
[2022-08-21 19:49:27,720] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,718 INFO storage.BlockManagerMasterEndpoint: Registering block manager rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 with 3.0 GiB RAM, BlockManagerId(1, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, 44987, None)
[2022-08-21 19:49:27,866] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,866 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('hdfs://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:8020/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:8020/user/hive/warehouse').
[2022-08-21 19:49:27,867] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,866 INFO internal.SharedState: Warehouse path is 'hdfs://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:8020/user/hive/warehouse'.
[2022-08-21 19:49:27,880] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,880 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[2022-08-21 19:49:27,881] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,881 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@798def2{/SQL,null,AVAILABLE,@Spark}
[2022-08-21 19:49:27,882] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,882 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[2022-08-21 19:49:27,883] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,883 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48f88851{/SQL/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:27,883] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,883 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[2022-08-21 19:49:27,884] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,884 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38d449c9{/SQL/execution,null,AVAILABLE,@Spark}
[2022-08-21 19:49:27,884] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,884 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[2022-08-21 19:49:27,885] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,885 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@448c4d5f{/SQL/execution/json,null,AVAILABLE,@Spark}
[2022-08-21 19:49:27,886] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,886 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
[2022-08-21 19:49:27,887] {spark_submit.py:485} INFO - 2022-08-21 19:49:27,887 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bec78df{/static/sql,null,AVAILABLE,@Spark}
[2022-08-21 19:49:28,663] {spark_submit.py:485} INFO - 2022-08-21 19:49:28,650 INFO datasources.InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
[2022-08-21 19:49:29,012] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,012 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2022-08-21 19:49:29,026] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,026 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-08-21 19:49:29,027] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,027 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2022-08-21 19:49:29,027] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,027 INFO scheduler.DAGScheduler: Parents of final stage: List()
[2022-08-21 19:49:29,029] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,029 INFO scheduler.DAGScheduler: Missing parents: List()
[2022-08-21 19:49:29,042] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,042 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-08-21 19:49:29,117] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,117 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 133.3 KiB, free 1643.1 MiB)
[2022-08-21 19:49:29,174] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,174 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 50.0 KiB, free 1643.1 MiB)
[2022-08-21 19:49:29,177] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,177 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 50.0 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:29,179] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,179 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
[2022-08-21 19:49:29,198] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,198 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-08-21 19:49:29,199] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,199 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks
[2022-08-21 19:49:29,238] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,238 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 0, PROCESS_LOCAL, 7564 bytes)
[2022-08-21 19:49:29,472] {spark_submit.py:485} INFO - 2022-08-21 19:49:29,472 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 50.0 KiB, free: 3.0 GiB)
[2022-08-21 19:49:30,830] {spark_submit.py:485} INFO - 2022-08-21 19:49:30,829 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1603 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (1/1)
[2022-08-21 19:49:30,833] {spark_submit.py:485} INFO - 2022-08-21 19:49:30,832 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-08-21 19:49:30,839] {spark_submit.py:485} INFO - 2022-08-21 19:49:30,838 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 1.778 s
[2022-08-21 19:49:30,847] {spark_submit.py:485} INFO - 2022-08-21 19:49:30,846 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-08-21 19:49:30,847] {spark_submit.py:485} INFO - 2022-08-21 19:49:30,847 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished
[2022-08-21 19:49:30,851] {spark_submit.py:485} INFO - 2022-08-21 19:49:30,850 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 1.838154 s
[2022-08-21 19:49:31,136] {spark_submit.py:485} INFO - 2022-08-21 19:49:31,136 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 in memory (size: 50.0 KiB, free: 1643.3 MiB)
[2022-08-21 19:49:31,170] {spark_submit.py:485} INFO - 2022-08-21 19:49:31,170 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 in memory (size: 50.0 KiB, free: 3.0 GiB)
[2022-08-21 19:49:33,442] {spark_submit.py:485} INFO - 2022-08-21 19:49:33,442 INFO datasources.FileSourceStrategy: Pushed Filters:
[2022-08-21 19:49:33,443] {spark_submit.py:485} INFO - 2022-08-21 19:49:33,443 INFO datasources.FileSourceStrategy: Post-Scan Filters:
[2022-08-21 19:49:33,445] {spark_submit.py:485} INFO - 2022-08-21 19:49:33,445 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TERMINAL_ID: bigint>
[2022-08-21 19:49:33,982] {spark_submit.py:485} INFO - 2022-08-21 19:49:33,981 INFO codegen.CodeGenerator: Code generated in 312.546844 ms
[2022-08-21 19:49:34,019] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,019 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 435.2 KiB, free 1642.8 MiB)
[2022-08-21 19:49:34,038] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,038 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 48.3 KiB, free 1642.8 MiB)
[2022-08-21 19:49:34,039] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,039 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 48.3 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:34,041] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,041 INFO spark.SparkContext: Created broadcast 1 from collect at StringIndexer.scala:204
[2022-08-21 19:49:34,052] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,052 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4556655 bytes, open cost is considered as scanning 4194304 bytes.
[2022-08-21 19:49:34,218] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,217 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204
[2022-08-21 19:49:34,222] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,221 INFO scheduler.DAGScheduler: Registering RDD 6 (collect at StringIndexer.scala:204) as input to shuffle 0
[2022-08-21 19:49:34,224] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,224 INFO scheduler.DAGScheduler: Got job 1 (collect at StringIndexer.scala:204) with 1 output partitions
[2022-08-21 19:49:34,224] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,224 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at StringIndexer.scala:204)
[2022-08-21 19:49:34,224] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,224 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2022-08-21 19:49:34,226] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,226 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
[2022-08-21 19:49:34,236] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,236 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204), which has no missing parents
[2022-08-21 19:49:34,261] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,261 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 23.3 KiB, free 1642.8 MiB)
[2022-08-21 19:49:34,263] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,263 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 1642.7 MiB)
[2022-08-21 19:49:34,264] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,264 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 10.5 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:34,265] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,265 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
[2022-08-21 19:49:34,269] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,269 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1))
[2022-08-21 19:49:34,269] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,269 INFO cluster.YarnScheduler: Adding task set 1.0 with 2 tasks
[2022-08-21 19:49:34,282] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,282 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 0, RACK_LOCAL, 7802 bytes)
[2022-08-21 19:49:34,283] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,283 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 1, RACK_LOCAL, 7802 bytes)
[2022-08-21 19:49:34,329] {spark_submit.py:485} INFO - 2022-08-21 19:49:34,328 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 10.5 KiB, free: 3.0 GiB)
[2022-08-21 19:49:35,036] {spark_submit.py:485} INFO - 2022-08-21 19:49:35,036 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 48.3 KiB, free: 3.0 GiB)
[2022-08-21 19:49:35,848] {spark_submit.py:485} INFO - 2022-08-21 19:49:35,848 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1566 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (1/2)
[2022-08-21 19:49:36,885] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,885 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2609 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (2/2)
[2022-08-21 19:49:36,885] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,885 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-08-21 19:49:36,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,886 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (collect at StringIndexer.scala:204) finished in 2.639 s
[2022-08-21 19:49:36,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,887 INFO scheduler.DAGScheduler: looking for newly runnable stages
[2022-08-21 19:49:36,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,888 INFO scheduler.DAGScheduler: running: Set()
[2022-08-21 19:49:36,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,888 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
[2022-08-21 19:49:36,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,888 INFO scheduler.DAGScheduler: failed: Set()
[2022-08-21 19:49:36,893] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,893 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at collect at StringIndexer.scala:204), which has no missing parents
[2022-08-21 19:49:36,902] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,902 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.2 KiB, free 1642.7 MiB)
[2022-08-21 19:49:36,925] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,925 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1642.7 MiB)
[2022-08-21 19:49:36,925] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,925 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 9.7 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:36,926] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,926 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
[2022-08-21 19:49:36,927] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,927 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
[2022-08-21 19:49:36,927] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,927 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks
[2022-08-21 19:49:36,929] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,929 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
[2022-08-21 19:49:36,951] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,950 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 9.7 KiB, free: 3.0 GiB)
[2022-08-21 19:49:36,996] {spark_submit.py:485} INFO - 2022-08-21 19:49:36,996 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.129.0.18:41230
[2022-08-21 19:49:37,167] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,166 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 238 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (1/1)
[2022-08-21 19:49:37,167] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,167 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-08-21 19:49:37,169] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,169 INFO scheduler.DAGScheduler: ResultStage 2 (collect at StringIndexer.scala:204) finished in 0.271 s
[2022-08-21 19:49:37,170] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,170 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-08-21 19:49:37,170] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,170 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished
[2022-08-21 19:49:37,171] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,171 INFO scheduler.DAGScheduler: Job 1 finished: collect at StringIndexer.scala:204, took 2.952993 s
[2022-08-21 19:49:37,217] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,217 INFO codegen.CodeGenerator: Code generated in 30.39453 ms
[2022-08-21 19:49:37,787] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,787 INFO datasources.FileSourceStrategy: Pushed Filters:
[2022-08-21 19:49:37,787] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,787 INFO datasources.FileSourceStrategy: Post-Scan Filters:
[2022-08-21 19:49:37,787] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,787 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TX_AMOUNT: double>
[2022-08-21 19:49:37,832] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,832 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 in memory (size: 9.7 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:37,841] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,841 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 in memory (size: 9.7 KiB, free: 3.0 GiB)
[2022-08-21 19:49:37,888] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,888 INFO codegen.CodeGenerator: Code generated in 45.131381 ms
[2022-08-21 19:49:37,901] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,901 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 435.2 KiB, free 1642.4 MiB)
[2022-08-21 19:49:37,903] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,903 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 in memory (size: 10.5 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:37,905] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,905 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 in memory (size: 10.5 KiB, free: 3.0 GiB)
[2022-08-21 19:49:37,925] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,925 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 48.3 KiB, free 1642.3 MiB)
[2022-08-21 19:49:37,927] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,926 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 48.3 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:37,927] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,927 INFO spark.SparkContext: Created broadcast 4 from first at MinMaxScaler.scala:120
[2022-08-21 19:49:37,928] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,928 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4556655 bytes, open cost is considered as scanning 4194304 bytes.
[2022-08-21 19:49:37,957] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,957 INFO spark.SparkContext: Starting job: first at MinMaxScaler.scala:120
[2022-08-21 19:49:37,960] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,959 INFO scheduler.DAGScheduler: Registering RDD 14 (first at MinMaxScaler.scala:120) as input to shuffle 1
[2022-08-21 19:49:37,960] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,959 INFO scheduler.DAGScheduler: Got job 2 (first at MinMaxScaler.scala:120) with 1 output partitions
[2022-08-21 19:49:37,960] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,960 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (first at MinMaxScaler.scala:120)
[2022-08-21 19:49:37,960] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,960 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2022-08-21 19:49:37,961] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,960 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
[2022-08-21 19:49:37,962] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,961 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at first at MinMaxScaler.scala:120), which has no missing parents
[2022-08-21 19:49:37,992] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,992 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.7 KiB, free 1642.3 MiB)
[2022-08-21 19:49:37,993] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,993 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 1642.3 MiB)
[2022-08-21 19:49:37,994] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,994 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 12.0 KiB, free: 1643.1 MiB)
[2022-08-21 19:49:37,995] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,995 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
[2022-08-21 19:49:37,995] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,995 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at first at MinMaxScaler.scala:120) (first 15 tasks are for partitions Vector(0, 1))
[2022-08-21 19:49:37,995] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,995 INFO cluster.YarnScheduler: Adding task set 3.0 with 2 tasks
[2022-08-21 19:49:37,997] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,997 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 0, RACK_LOCAL, 7802 bytes)
[2022-08-21 19:49:37,998] {spark_submit.py:485} INFO - 2022-08-21 19:49:37,998 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 1, RACK_LOCAL, 7802 bytes)
[2022-08-21 19:49:38,018] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,018 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 12.0 KiB, free: 3.0 GiB)
[2022-08-21 19:49:38,170] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,170 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 48.3 KiB, free: 3.0 GiB)
[2022-08-21 19:49:38,260] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,260 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 263 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (1/2)
[2022-08-21 19:49:38,774] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,773 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 776 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (2/2)
[2022-08-21 19:49:38,774] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,773 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-08-21 19:49:38,775] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,775 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (first at MinMaxScaler.scala:120) finished in 0.812 s
[2022-08-21 19:49:38,775] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,775 INFO scheduler.DAGScheduler: looking for newly runnable stages
[2022-08-21 19:49:38,775] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,775 INFO scheduler.DAGScheduler: running: Set()
[2022-08-21 19:49:38,775] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,775 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
[2022-08-21 19:49:38,775] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,775 INFO scheduler.DAGScheduler: failed: Set()
[2022-08-21 19:49:38,776] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,775 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at first at MinMaxScaler.scala:120), which has no missing parents
[2022-08-21 19:49:38,785] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,785 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.7 KiB, free 1642.2 MiB)
[2022-08-21 19:49:38,787] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,786 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 1642.2 MiB)
[2022-08-21 19:49:38,788] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,788 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 10.2 KiB, free: 1643.1 MiB)
[2022-08-21 19:49:38,788] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,788 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
[2022-08-21 19:49:38,789] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,789 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at first at MinMaxScaler.scala:120) (first 15 tasks are for partitions Vector(0))
[2022-08-21 19:49:38,790] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,790 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks
[2022-08-21 19:49:38,792] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,792 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 0, NODE_LOCAL, 7336 bytes)
[2022-08-21 19:49:38,805] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,805 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 10.2 KiB, free: 3.0 GiB)
[2022-08-21 19:49:38,819] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,818 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.129.0.18:41230
[2022-08-21 19:49:38,883] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,882 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 91 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (1/1)
[2022-08-21 19:49:38,883] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,883 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-08-21 19:49:38,888] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,888 INFO scheduler.DAGScheduler: ResultStage 4 (first at MinMaxScaler.scala:120) finished in 0.112 s
[2022-08-21 19:49:38,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,889 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-08-21 19:49:38,889] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,889 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished
[2022-08-21 19:49:38,891] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,890 INFO scheduler.DAGScheduler: Job 2 finished: first at MinMaxScaler.scala:120, took 0.932498 s
[2022-08-21 19:49:38,918] {spark_submit.py:485} INFO - 2022-08-21 19:49:38,917 INFO codegen.CodeGenerator: Code generated in 16.608202 ms
[2022-08-21 19:49:39,130] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,130 INFO datasources.FileSourceStrategy: Pushed Filters:
[2022-08-21 19:49:39,131] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,131 INFO datasources.FileSourceStrategy: Post-Scan Filters:
[2022-08-21 19:49:39,131] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,131 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TERMINAL_ID: bigint, TX_AMOUNT: double, TX_FRAUD: bigint ... 1 more fields>
[2022-08-21 19:49:39,197] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,197 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-08-21 19:49:39,220] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,220 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-08-21 19:49:39,220] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,220 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-08-21 19:49:39,221] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,221 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-08-21 19:49:39,222] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,222 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-08-21 19:49:39,222] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,222 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-08-21 19:49:39,223] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,223 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-08-21 19:49:39,313] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,313 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 in memory (size: 10.2 KiB, free: 1643.1 MiB)
[2022-08-21 19:49:39,317] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,316 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 in memory (size: 10.2 KiB, free: 3.0 GiB)
[2022-08-21 19:49:39,347] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,346 INFO codegen.CodeGenerator: Code generated in 98.534992 ms
[2022-08-21 19:49:39,354] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,353 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 435.7 KiB, free 1641.8 MiB)
[2022-08-21 19:49:39,369] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,368 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 in memory (size: 48.3 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:39,371] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,371 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 1642.3 MiB)
[2022-08-21 19:49:39,372] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,372 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 48.4 KiB, free: 1643.1 MiB)
[2022-08-21 19:49:39,374] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,374 INFO spark.SparkContext: Created broadcast 7 from save at NativeMethodAccessorImpl.java:0
[2022-08-21 19:49:39,375] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,375 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4556655 bytes, open cost is considered as scanning 4194304 bytes.
[2022-08-21 19:49:39,385] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,385 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 in memory (size: 48.3 KiB, free: 3.0 GiB)
[2022-08-21 19:49:39,417] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,417 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 in memory (size: 12.0 KiB, free: 1643.2 MiB)
[2022-08-21 19:49:39,419] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,419 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 in memory (size: 12.0 KiB, free: 3.0 GiB)
[2022-08-21 19:49:39,428] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,428 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2022-08-21 19:49:39,430] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,430 INFO scheduler.DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2022-08-21 19:49:39,430] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,430 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
[2022-08-21 19:49:39,430] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,430 INFO scheduler.DAGScheduler: Parents of final stage: List()
[2022-08-21 19:49:39,430] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,430 INFO scheduler.DAGScheduler: Missing parents: List()
[2022-08-21 19:49:39,435] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,435 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-08-21 19:49:39,487] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,487 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 315.1 KiB, free 1642.0 MiB)
[2022-08-21 19:49:39,490] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,490 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 114.4 KiB, free 1641.9 MiB)
[2022-08-21 19:49:39,491] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,491 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:33851 (size: 114.4 KiB, free: 1643.0 MiB)
[2022-08-21 19:49:39,492] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,492 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
[2022-08-21 19:49:39,493] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,492 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2022-08-21 19:49:39,493] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,493 INFO cluster.YarnScheduler: Adding task set 5.0 with 2 tasks
[2022-08-21 19:49:39,496] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,495 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 0, RACK_LOCAL, 7813 bytes)
[2022-08-21 19:49:39,496] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,496 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8, rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net, executor 1, partition 1, RACK_LOCAL, 7813 bytes)
[2022-08-21 19:49:39,539] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,525 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 114.4 KiB, free: 3.0 GiB)
[2022-08-21 19:49:39,803] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,803 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net:44987 (size: 48.4 KiB, free: 3.0 GiB)
[2022-08-21 19:49:39,911] {spark_submit.py:485} INFO - 2022-08-21 19:49:39,911 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 416 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (1/2)
[2022-08-21 19:49:40,986] {spark_submit.py:485} INFO - 2022-08-21 19:49:40,986 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 1492 ms on rc1b-dataproc-d-huy8cajc5udgv1rf.mdb.yandexcloud.net (executor 1) (2/2)
[2022-08-21 19:49:40,986] {spark_submit.py:485} INFO - 2022-08-21 19:49:40,986 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-08-21 19:49:40,988] {spark_submit.py:485} INFO - 2022-08-21 19:49:40,987 INFO scheduler.DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 1.550 s
[2022-08-21 19:49:40,988] {spark_submit.py:485} INFO - 2022-08-21 19:49:40,987 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-08-21 19:49:40,988] {spark_submit.py:485} INFO - 2022-08-21 19:49:40,987 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished
[2022-08-21 19:49:40,988] {spark_submit.py:485} INFO - 2022-08-21 19:49:40,987 INFO scheduler.DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 1.558755 s
[2022-08-21 19:49:41,011] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,010 INFO datasources.FileFormatWriter: Write Job 76a6f18c-f6ed-4014-b932-0115f79aa93e committed.
[2022-08-21 19:49:41,014] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,014 INFO datasources.FileFormatWriter: Finished processing stats for write job 76a6f18c-f6ed-4014-b932-0115f79aa93e.
[2022-08-21 19:49:41,075] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,075 INFO spark.SparkContext: Invoking stop() from shutdown hook
[2022-08-21 19:49:41,084] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,084 INFO server.AbstractConnector: Stopped Spark@3a673000{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
[2022-08-21 19:49:41,085] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,085 INFO ui.SparkUI: Stopped Spark web UI at http://rc1b-dataproc-m-xzxwmqcudfo0foh0.mdb.yandexcloud.net:4040
[2022-08-21 19:49:41,090] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,090 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
[2022-08-21 19:49:41,099] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,099 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
[2022-08-21 19:49:41,100] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,100 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
[2022-08-21 19:49:41,105] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,105 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped
[2022-08-21 19:49:41,130] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,129 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-08-21 19:49:41,158] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,156 INFO memory.MemoryStore: MemoryStore cleared
[2022-08-21 19:49:41,159] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,156 INFO storage.BlockManager: BlockManager stopped
[2022-08-21 19:49:41,173] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,172 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
[2022-08-21 19:49:41,176] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,176 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-08-21 19:49:41,209] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,208 INFO spark.SparkContext: Successfully stopped SparkContext
[2022-08-21 19:49:41,209] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,209 INFO util.ShutdownHookManager: Shutdown hook called
[2022-08-21 19:49:41,210] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,210 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-166af703-d09d-48c7-9d05-6c040db15f5a
[2022-08-21 19:49:41,217] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,217 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d80d3c76-e708-4e9a-9618-ad111728fc5c
[2022-08-21 19:49:41,220] {spark_submit.py:485} INFO - 2022-08-21 19:49:41,220 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d80d3c76-e708-4e9a-9618-ad111728fc5c/pyspark-89357a74-ac44-42f6-bd84-a0b98d0cc999
[2022-08-21 19:49:41,577] {taskinstance.py:1415} INFO - Marking task as SUCCESS. dag_id=preprocessing_data, task_id=clean_data, execution_date=20220821T193913, start_date=20220821T194916, end_date=20220821T194941
[2022-08-21 19:49:41,604] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-08-21 19:49:41,696] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
